<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects |</title><link>https://drparkar.github.io/project/</link><atom:link href="https://drparkar.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 02 Oct 2023 00:00:00 +0000</lastBuildDate><image><url>https://drparkar.github.io/media/icon_hu0ae117fe047177e402e0184950c2f09a_3600_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://drparkar.github.io/project/</link></image><item><title>DMAR: Decentralized Multi-agent Rollout Algorithm</title><link>https://drparkar.github.io/project/dec-mahr/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://drparkar.github.io/project/dec-mahr/</guid><description>&lt;p>
&lt;video controls >
&lt;source src="https://drparkar.github.io/media/decMAHR_robo_exp.mp4" type="video/mp4">
&lt;/video>
A short demonstration of the algorithm executed on the robotarium testbed with physical robots. The outlined shapes around robots denote the cluster they are part of and the controls are executed in parallel.&lt;/p>
&lt;p>&lt;strong>DMAR(Decentralized Multi-Agent Heuristic Rollout)&lt;/strong> algorithm is a newly proposed reinforcement learning algorithm that solves a hitherto unsolved variant of vehicle routing problem; multi-vehicle routing problem in unmapped environments. The developed DMAR algorithm solves it with provable guarantees of cost improvements over a greedy base policy (which is the only viable and probabilistically complete policy aside from DMAR).&lt;/p>
&lt;p>The DecMAHR is considered even more powerful due to the fact that the execution is online and completely decentralized. This makes it feasible to be deployed on physical robots and makes it robust for real-world solutions using robots.&lt;/p></description></item><item><title>EvoSOPS: Evolving global collective behaviors using restricted agents</title><link>https://drparkar.github.io/project/ssops-ga/</link><pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate><guid>https://drparkar.github.io/project/ssops-ga/</guid><description>&lt;p>Can we automate the process of finding algorithms that can induce global behaviors in a collective of particles? This research project that I started with Kirtus Leyba and Prof. Joshua Daymude aims to answer this question.&lt;/p>
&lt;p>We are using the &lt;em>Self-organizing particle systems(SOPS)&lt;/em> model developed at &lt;a href="https://sops.engineering.asu.edu/sops/" target="_blank" rel="noopener">SOPS Lab(ASU)&lt;/a> to study distributed collective algorithms. It consists of individual, homogeneous computational elements called particles which move along the vertices of a triangular lattice structure. The algorithms we develop are stochastic in nature and encode markov-chains that govern the movement dynamics of the particles. These algorithms only rely on local information available to the particles and operate in distributed paradigm.&lt;/p>
&lt;p>For optimizing and searching for solution algorithms Genetic Algorithms are used. Genetic Algorithms provide for a heuristic based optimization which can balance out exploration and exploitation. We need such robustness and flexibility in optimization since we require our algorithms to be agnostic to the number of particles and find multiple solution algorithms if they exist.&lt;/p></description></item><item><title>IARPA HAYSTACK - Anamoly Detection</title><link>https://drparkar.github.io/project/haystack/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://drparkar.github.io/project/haystack/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>PyReason-Gym: Symbolic logic simulator for Reinforcement Learning</title><link>https://drparkar.github.io/project/pyreason-gym-rl/</link><pubDate>Sun, 02 Jul 2023 00:00:00 +0000</pubDate><guid>https://drparkar.github.io/project/pyreason-gym-rl/</guid><description>
&lt;video controls >
&lt;source src="https://drparkar.github.io/media/pyreason-gym-sim.mov" type="video/quicktime">
&lt;/video>
&lt;p>&lt;strong>PyReason-Gym&lt;/strong> is a new symbolic simulator introduced to enable symbolic reinforcement learning. Why is such a simulator necessary? Well, this let&amp;rsquo;s us learn interpretable agent policies defacto and enable transfering of policies to other simulation environments. We explored these possibilities and worked on the project with following aims:&lt;/p>
&lt;ol>
&lt;li>Introduce and verify the new PyReason-Gym simulator for symbolic reinforcement learning.&lt;/li>
&lt;li>Extend exisiting deep reinforcement learning algorithms to take advantage of symbolic representations provided by PyReason-Gym&lt;/li>
&lt;li>Demonstrate ease of incorporating human-level symbolic logic to enable reinforcement learning in non-markovian dynamics(temporally non-markovian), which existing algorithms struggle with&lt;/li>
&lt;li>Demonstrate ease of interpreting and transfering learned policies to other simulation environments&lt;/li>
&lt;/ol>
&lt;p>More about the simulator and underlying inference engine can be found below:&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
&lt;ul>
&lt;li>&lt;a href="https://github.com/lab-v2/pyreason-gym" target="_blank" rel="noopener">PyReason-Gym python package&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/lab-v2/pyreason" target="_blank" rel="noopener">PyReason python package&lt;/a>&lt;/li>
&lt;li>A conference paper is submitted and under review&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div></description></item><item><title>Neuro-Evolutionary Swarms</title><link>https://drparkar.github.io/project/neuro-evo-swarms/</link><pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate><guid>https://drparkar.github.io/project/neuro-evo-swarms/</guid><description>
&lt;video controls >
&lt;source src="https://drparkar.github.io/media/neuroEvo_exp.mp4" type="video/mp4">
&lt;/video>
&lt;p>&lt;strong>Background:&lt;/strong> Initially, the project was supposed to be a simple understanding and implementation of existing static swarm behaviors studied by others. Later I grew curious to see if there was a way to evolve dynamic behaviors just like nature does. I understood the power of neural networks and paired it with genetic algorithms to see if a dynamic robot controller could result in complex behaviors. I tested out a hypothesis that inter-dependent evolutionary pressures drives the complex behavior of communication to evolve in animals. I successfully managed to develop a nascent form of communication, where prey robots where able to alert other prey robots about predator robots in the vicinity.&lt;/p>
&lt;p>&lt;strong>Experiments:&lt;/strong> ARGoS multi-robot simulator was used to conduct experiments on Foot-bot robots (part of Swarmanoid project) which had following capabilities:&lt;/p>
&lt;ol>
&lt;li>RGB LEDs&lt;/li>
&lt;li>Omnidirectional camera&lt;/li>
&lt;li>Proximity sensor&lt;/li>
&lt;/ol>
&lt;p>It was observed that prey bots rotated about their axis and maintained equal distance from each other so as to increase the scope of detecting a predator bot. The predator bot evolved to chase a prey that was closest to it however due to wider vision it would usually end the chase pre-maturely.
The preys evolved a nascent communication protocol whereby they started blinking whenever they were chased by a predator and were in the vicinity of other preys. Any prey bots which detected the signal moved away quickly and did the same to notify other prey bots. The experiments gave a proof of the earlier hypothesis and showed that neuro-evolutionary models can be used to build complex behaviors&lt;/p></description></item></channel></rss>